{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debff8c6",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c7903",
   "metadata": {},
   "source": [
    "Loading STL-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e06dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "stl10_train = torchvision.datasets.STL10(root='./stl_data', split='train', download=True, transform=transforms)\n",
    "stl10_test = torchvision.datasets.STL10(root='./stl_data', split='test', download=True, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e54cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(stl10_train))\n",
    "val_size = len(stl10_train) - train_size\n",
    "stl10_train_split, stl10_val_split = random_split(stl10_train, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5dda3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d107483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(stl10_train_split, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(stl10_val_split, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(stl10_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ae934",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23693ae",
   "metadata": {},
   "source": [
    "Loading ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e146eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
    "resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f313bd56",
   "metadata": {},
   "source": [
    "Loading ViT-S/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb17c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.DEFAULT)\n",
    "vit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837cc79e",
   "metadata": {},
   "source": [
    "Fine-tuning ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ca31a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACING FINAL LAYER\n",
    "resnet.fc = torch.nn.Linear(in_features=resnet.fc.in_features, out_features=stl10_train.classes, bias=True) \n",
    "\n",
    "# FREEZING BACKBONE\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in resnet.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "resnet = resnet.to(DEVICE)\n",
    "EPOCHS = 10\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet.fc.parameters(), lr=0.005)\n",
    "\n",
    "# FULL FINETUNING\n",
    "# for param in resnet.parameters():\n",
    "#     param.requires_grad = True\n",
    "# optimizer = optim.Adam(resnet.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9781e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_train_accuracies = []\n",
    "resnet_train_losses = []\n",
    "resnet_val_accuracies = []\n",
    "resnet_val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    resnet.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    train_acc = 100. * correct / total\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    resnet_train_accuracies.append(train_acc)\n",
    "    resnet_train_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"\\tTraining Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "    # Validation\n",
    "    resnet.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = resnet(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    val_acc = 100. * correct / total\n",
    "    avg_val_loss = val_running_loss / len(val_loader)\n",
    "    resnet_val_accuracies.append(val_acc)\n",
    "    resnet_val_losses.append(avg_val_loss)\n",
    "    print(f\"\\tValidation Loss: {avg_val_loss:.4f}, Validation Acc: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04fd281",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet.state_dict(), 'resnet_finetuned.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055aae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.load_state_dict(torch.load('resnet_finetuned.pth', map_location=DEVICE))\n",
    "print(\"Loaded fine-tuned ResNet-50 weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d48dc",
   "metadata": {},
   "source": [
    "Fine-tuning ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfdd7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit.heads.head = torch.nn.Linear(vit.heads.head.in_features, 10)\n",
    "\n",
    "# Freeze backbone, only train classification head\n",
    "for param in vit.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in vit.heads.head.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "\n",
    "vit = vit.to(DEVICE)\n",
    "EPOCHS = 10\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vit.heads.head.parameters(), lr=0.005)\n",
    "\n",
    "# FULL FINETUNING\n",
    "# for param in vit.parameters():\n",
    "#     param.requires_grad = True\n",
    "# optimizer = optim.Adam(vit.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ce363",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_train_accuracies = []\n",
    "vit_train_losses = []\n",
    "vit_val_accuracies = []\n",
    "vit_val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    vit.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vit(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    train_acc = 100. * correct / total\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    vit_train_accuracies.append(train_acc)\n",
    "    vit_train_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"\\tTraining Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "    # Validation\n",
    "    vit.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = vit(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    val_acc = 100. * correct / total\n",
    "    avg_val_loss = val_running_loss / len(val_loader)\n",
    "    vit_val_accuracies.append(val_acc)\n",
    "    vit_val_losses.append(avg_val_loss)\n",
    "    print(f\"\\tValidation Loss: {avg_val_loss:.4f}, Validation Acc: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec60514",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vit.state_dict(), 'vit_finetuned.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc0575",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit.load_state_dict(torch.load('vit_finetuned.pth', map_location=DEVICE))\n",
    "print(\"Loaded fine-tuned ViT weights.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2712cb",
   "metadata": {},
   "source": [
    "## In-Distribution Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d9a887",
   "metadata": {},
   "source": [
    "Evaluating Fine-Tuned ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.eval()\n",
    "resnet_test_loss = 0.0\n",
    "resnet_correct = 0\n",
    "resnet_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = resnet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        resnet_test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        resnet_total += labels.size(0)\n",
    "        resnet_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "resnet_test_acc = 100. * resnet_correct / resnet_total\n",
    "resnet_avg_test_loss = resnet_test_loss / len(test_loader)\n",
    "print(f\"ResNet-50 Test Loss: {resnet_avg_test_loss:.4f}, Test Acc: {resnet_test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2401ff",
   "metadata": {},
   "source": [
    "Evaluating Fine-Tuned ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421bf5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit.eval()\n",
    "vit_test_loss = 0.0\n",
    "vit_correct = 0\n",
    "vit_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = vit(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        vit_test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        vit_total += labels.size(0)\n",
    "        vit_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "vit_test_acc = 100. * vit_correct / vit_total\n",
    "vit_avg_test_loss = vit_test_loss / len(test_loader)\n",
    "print(f\"ViT-S/16 Test Loss: {vit_avg_test_loss:.4f}, Test Acc: {vit_test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeb308e",
   "metadata": {},
   "source": [
    "## Color-bias Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a297521",
   "metadata": {},
   "source": [
    "Grayscale Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6413a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_grayscale = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Grayscale(num_output_channels=3),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "stl10_test_grayscale = torchvision.datasets.STL10(root='./stl_data', split='test', download=True, transform=transforms_grayscale)\n",
    "test_loader = DataLoader(stl10_test_grayscale, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1633826e",
   "metadata": {},
   "source": [
    "Evaluating Fine-Tuned ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41951073",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.eval()\n",
    "resnet_gray_correct = 0\n",
    "resnet_gray_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = resnet(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        resnet_gray_total += labels.size(0)\n",
    "        resnet_gray_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "resnet_gray_acc = 100. * resnet_gray_correct / resnet_gray_total\n",
    "print(f\"ResNet-50 Grayscale Test Acc: {resnet_gray_acc:.2f}%\")\n",
    "print(f\"ResNet-50 Accuracy Drop: {resnet_test_acc - resnet_gray_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3145129",
   "metadata": {},
   "source": [
    "Evaluating Fine-Tuned ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06fd468",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit.eval()\n",
    "vit_gray_correct = 0\n",
    "vit_gray_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = vit(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        vit_gray_total += labels.size(0)\n",
    "        vit_gray_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "vit_gray_acc = 100. * vit_gray_correct / vit_gray_total\n",
    "print(f\"ViT-S/16 Grayscale Test Acc: {vit_gray_acc:.2f}%\")\n",
    "print(f\"ViT-S/16 Accuracy Drop: {vit_test_acc - vit_gray_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c09ce57",
   "metadata": {},
   "source": [
    "## Shape vs. Texture Bias â€“ Stylized Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a73155",
   "metadata": {},
   "source": [
    "Use this repo: https://github.com/rgeirhos/Stylized-ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59cda59",
   "metadata": {},
   "source": [
    "## Translation Invariance Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f66eb48",
   "metadata": {},
   "source": [
    "Translated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83334556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_shifted_test_loader(shift_x=0, shift_y=0):\n",
    "#     shift_transform = torchvision.transforms.Compose([\n",
    "#         torchvision.transforms.ToPILImage(),\n",
    "#         torchvision.transforms.functional.Lambda(lambda img: torchvision.transforms.functional.affine(img, angle=0, translate=(shift_x, shift_y), scale=1.0, shear=0)),\n",
    "#         torchvision.transforms.ToTensor(),\n",
    "#     ])\n",
    "#     shifted_dataset = torchvision.datasets.STL10(\n",
    "#         root='./stl_data',\n",
    "#         split='test',\n",
    "#         download=True,\n",
    "#         transform=lambda img: shift_transform(img)\n",
    "#     )\n",
    "#     return DataLoader(shifted_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# shifted_loader = get_shifted_test_loader(shift_x=5, shift_y=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e0a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHIFT_PIXELS = 5\n",
    "\n",
    "DIRECTIONS = {\n",
    "    \"up\": (0, -SHIFT_PIXELS),\n",
    "    \"down\": (0, SHIFT_PIXELS),\n",
    "    \"left\": (-SHIFT_PIXELS, 0),\n",
    "    \"right\": (SHIFT_PIXELS, 0),\n",
    "    \"up_left\": (-SHIFT_PIXELS, -SHIFT_PIXELS),\n",
    "    \"up_right\": (SHIFT_PIXELS, -SHIFT_PIXELS),\n",
    "    \"down_left\": (-SHIFT_PIXELS, SHIFT_PIXELS),\n",
    "    \"down_right\": (SHIFT_PIXELS, SHIFT_PIXELS),\n",
    "}\n",
    "\n",
    "def make_shift_transform(dx, dy, fill=0):\n",
    "    return torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Lambda(lambda img: TF.affine(\n",
    "            img,\n",
    "            angle=0,\n",
    "            translate=(dx, dy),\n",
    "            scale=1.0,\n",
    "            shear=0.0,\n",
    "            interpolation=torchvision.transforms.InterpolationMode.BILINEAR,\n",
    "            fill=fill\n",
    "        )),\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# class RollShiftToTensor:\n",
    "#     \"\"\"Tensor-space circular shift (wrap-around); no padding borders.\"\"\"\n",
    "#     def __init__(self, dx, dy):\n",
    "#         self.dx, self.dy = dx, dy\n",
    "#     def __call__(self, img_pil):\n",
    "#         x = torchvision.transforms.ToTensor()(img_pil)          # [C,H,W]\n",
    "#         x = torch.roll(x, shifts=(self.dy, self.dx), dims=(1, 2))  # (H,W)\n",
    "#         return x\n",
    "    \n",
    "class RandomShiftSTL10(torchvision.datasets.STL10):\n",
    "    def __init__(self, root, split, download, directions, fill=0):\n",
    "        super().__init__(root=root, split=split, download=download, transform=None)\n",
    "        self.directions = list(directions.items())\n",
    "        self.fill = fill\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = super().__getitem__(index) \n",
    "        name, (dx, dy) = random.choice(self.directions)\n",
    "        img = TF.affine(\n",
    "            img, angle=0, translate=(dx, dy), scale=1.0, shear=0.0,\n",
    "            interpolation=torchvision.transforms.InterpolationMode.BILINEAR, fill=self.fill\n",
    "        )\n",
    "        return torchvision.transforms.ToTensor()(img), target, name\n",
    "        # return RollShiftToTensor(self.dx, self.dy)(img), target, self.direction_name\n",
    "\n",
    "def get_mixed_shift_loader(split=\"test\", batch_size=BATCH_SIZE, fill=0, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "    ds = RandomShiftSTL10(root=\"./stl_data\", split=split, download=True,\n",
    "                          directions=DIRECTIONS, fill=fill)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "shifted_loader = get_mixed_shift_loader(split=\"test\", seed=1337)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ca6cc",
   "metadata": {},
   "source": [
    "Evaluating ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a5f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.eval()\n",
    "resnet_shift_correct = 0\n",
    "resnet_shift_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in shifted_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = resnet(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        resnet_shift_total += labels.size(0)\n",
    "        resnet_shift_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "resnet_shift_acc = 100. * resnet_shift_correct / resnet_shift_total\n",
    "print(f\"ResNet-50 Shifted Test Acc: {resnet_shift_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2216fb",
   "metadata": {},
   "source": [
    "Evaluating ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit.eval()\n",
    "vit_shift_correct = 0\n",
    "vit_shift_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in shifted_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = vit(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        vit_shift_total += labels.size(0)\n",
    "        vit_shift_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "vit_shift_acc = 100. * vit_shift_correct / vit_shift_total\n",
    "print(f\"ViT-S/16 Shifted Test Acc: {vit_shift_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6103bf2e",
   "metadata": {},
   "source": [
    "## Permutation / Occlusion Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1023884c",
   "metadata": {},
   "source": [
    "## Feature Representation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a57ac87",
   "metadata": {},
   "source": [
    "## Domain Generalization Test on PACS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354984c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Load PACS dataset from Hugging Face\n",
    "ds = load_dataset(\"flwrlabs/pacs\")\n",
    "print(f\"Dataset structure: {ds}\")\n",
    "print(f\"Available domains: {ds['train'].features['domain'].names}\")\n",
    "print(f\"Available classes: {ds['train'].features['label'].names}\")\n",
    "\n",
    "# Create a PyTorch dataset wrapper for the Hugging Face dataset\n",
    "class PACSDatasetWrapper(Dataset):\n",
    "    def __init__(self, hf_dataset, domains, transform=None):\n",
    "        self.transform = transform\n",
    "        # Filter for selected domains\n",
    "        self.data = hf_dataset.filter(lambda example: example['domain'] in domains)\n",
    "        self.classes = hf_dataset.features['label'].names\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = item['image'].convert('RGB')\n",
    "        label = item['label']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define train and test domains\n",
    "train_domains = ['photo', 'art_painting', 'cartoon']\n",
    "test_domain = ['sketch']\n",
    "\n",
    "# Create datasets\n",
    "pacs_train = PACSDatasetWrapper(ds['train'], domains=train_domains, transform=transform)\n",
    "pacs_test = PACSDatasetWrapper(ds['train'], domains=test_domain, transform=transform)\n",
    "\n",
    "# Split training data into train and validation\n",
    "train_size = int(0.9 * len(pacs_train))\n",
    "val_size = len(pacs_train) - train_size\n",
    "pacs_train_split, pacs_val_split = random_split(pacs_train, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "pacs_train_loader = DataLoader(pacs_train_split, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "pacs_val_loader = DataLoader(pacs_val_split, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "pacs_test_loader = DataLoader(pacs_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Training on {len(pacs_train_split)} images from domains: {', '.join(train_domains)}\")\n",
    "print(f\"Validating on {len(pacs_val_split)} images from domains: {', '.join(train_domains)}\")\n",
    "print(f\"Testing on {len(pacs_test)} images from domain: {', '.join(test_domain)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db360f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PACS dataset\n",
    "def download_pacs():\n",
    "    url = 'https://drive.google.com/uc?export=download&id=1JFr8f805nMUelQWWmfnJR3y75PlRmGCJ'\n",
    "    if not os.path.exists('./pacs_data'):\n",
    "        os.makedirs('./pacs_data')\n",
    "    if not os.path.exists('./pacs_data/PACS.tar.gz'):\n",
    "        print(\"Downloading PACS dataset...\")\n",
    "        urllib.request.urlretrieve(url, './pacs_data/PACS.tar.gz')\n",
    "        \n",
    "    if not os.path.exists('./pacs_data/PACS'):\n",
    "        print(\"Extracting PACS dataset...\")\n",
    "        tar = tarfile.open('./pacs_data/PACS.tar.gz')\n",
    "        tar.extractall('./pacs_data')\n",
    "        tar.close()\n",
    "    print(\"PACS dataset ready\")\n",
    "\n",
    "# Custom dataset class for PACS\n",
    "class PACSDataset(Dataset):\n",
    "    def __init__(self, root_dir, domains, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        \n",
    "        # PACS dataset has 7 classes\n",
    "        self.classes = ['dog', 'elephant', 'giraffe', 'guitar', 'horse', 'house', 'person']\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "        # Load images from the specified domains\n",
    "        for domain in domains:\n",
    "            domain_dir = os.path.join(root_dir, domain)\n",
    "            for class_name in os.listdir(domain_dir):\n",
    "                if class_name in self.classes:\n",
    "                    class_dir = os.path.join(domain_dir, class_name)\n",
    "                    for img_name in os.listdir(class_dir):\n",
    "                        if img_name.endswith('.jpg') or img_name.endswith('.png'):\n",
    "                            self.samples.append((\n",
    "                                os.path.join(class_dir, img_name),\n",
    "                                self.class_to_idx[class_name]\n",
    "                            ))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Download the dataset\n",
    "download_pacs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0bc2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ResNet and ViT expect 224x224 images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Prepare datasets\n",
    "train_domains = ['photo', 'art_painting', 'cartoon']\n",
    "test_domain = ['sketch']\n",
    "\n",
    "pacs_train = PACSDataset(\n",
    "    root_dir='./pacs_data/PACS/kfold',\n",
    "    domains=train_domains,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "pacs_test = PACSDataset(\n",
    "    root_dir='./pacs_data/PACS/kfold',\n",
    "    domains=test_domain,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Split training data into train and validation\n",
    "train_size = int(0.9 * len(pacs_train))\n",
    "val_size = len(pacs_train) - train_size\n",
    "pacs_train_split, pacs_val_split = random_split(pacs_train, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "pacs_train_loader = DataLoader(pacs_train_split, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "pacs_val_loader = DataLoader(pacs_val_split, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "pacs_test_loader = DataLoader(pacs_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Training on {len(pacs_train_split)} images from domains: {', '.join(train_domains)}\")\n",
    "print(f\"Validating on {len(pacs_val_split)} images from domains: {', '.join(train_domains)}\")\n",
    "print(f\"Testing on {len(pacs_test)} images from domain: {', '.join(test_domain)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5310dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, dataloader, criterion, optimizer, epochs=5):\n",
    "    train_accuracies = []\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        train_acc = 100. * correct / total\n",
    "        avg_train_loss = running_loss / len(dataloader)\n",
    "        train_accuracies.append(train_acc)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        print(f\"\\tTraining Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in pacs_val_loader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_acc = 100. * correct / total\n",
    "        avg_val_loss = val_running_loss / len(pacs_val_loader)\n",
    "        val_accuracies.append(val_acc)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"\\tValidation Loss: {avg_val_loss:.4f}, Validation Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    return train_accuracies, train_losses, val_accuracies, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_pacs = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
    "resnet_pacs.fc = torch.nn.Linear(resnet_pacs.fc.in_features, len(pacs_train.classes))\n",
    "\n",
    "# Freeze backbone, only train the last layer\n",
    "for param in resnet_pacs.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in resnet_pacs.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "resnet_pacs = resnet_pacs.to(DEVICE)\n",
    "resnet_pacs_criterion = torch.nn.CrossEntropyLoss()\n",
    "resnet_pacs_optimizer = optim.Adam(resnet_pacs.fc.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef85cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fine-tuning ResNet-50 on PACS dataset...\")\n",
    "resnet_pacs_train_acc, resnet_pacs_train_loss, resnet_pacs_val_acc, resnet_pacs_val_loss = train_model(\n",
    "    model=resnet_pacs,\n",
    "    dataloader=pacs_train_loader,\n",
    "    criterion=resnet_pacs_criterion,\n",
    "    optimizer=resnet_pacs_optimizer,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef3ce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune ViT for PACS\n",
    "vit_pacs = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.DEFAULT)\n",
    "vit_pacs.heads.head = torch.nn.Linear(vit_pacs.heads.head.in_features, len(pacs_train.classes))\n",
    "\n",
    "# Freeze backbone, only train classification head\n",
    "for param in vit_pacs.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in vit_pacs.heads.head.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "vit_pacs = vit_pacs.to(DEVICE)\n",
    "vit_pacs_criterion = torch.nn.CrossEntropyLoss()\n",
    "vit_pacs_optimizer = optim.Adam(vit_pacs.heads.head.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e3e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ViT on PACS\n",
    "print(\"Fine-tuning ViT-B/16 on PACS dataset...\")\n",
    "vit_pacs_train_acc, vit_pacs_train_loss, vit_pacs_val_acc, vit_pacs_val_loss = train_model(\n",
    "    model=vit_pacs,\n",
    "    dataloader=pacs_train_loader,\n",
    "    criterion=vit_pacs_criterion,\n",
    "    optimizer=vit_pacs_optimizer,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72388538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ResNet on Sketch domain\n",
    "resnet_pacs.eval()\n",
    "resnet_sketch_correct = 0\n",
    "resnet_sketch_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in pacs_test_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = resnet_pacs(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        resnet_sketch_total += labels.size(0)\n",
    "        resnet_sketch_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "resnet_sketch_acc = 100. * resnet_sketch_correct / resnet_sketch_total\n",
    "print(f\"ResNet-50 Sketch Domain Acc: {resnet_sketch_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8183bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate ViT on Sketch domain\n",
    "vit_pacs.eval()\n",
    "vit_sketch_correct = 0\n",
    "vit_sketch_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in pacs_test_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = vit_pacs(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        vit_sketch_total += labels.size(0)\n",
    "        vit_sketch_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "vit_sketch_acc = 100. * vit_sketch_correct / vit_sketch_total\n",
    "print(f\"ViT-B/16 Sketch Domain Acc: {vit_sketch_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c1868",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(resnet_pacs_train_acc, label='ResNet Train')\n",
    "plt.plot(vit_pacs_train_acc, label='ViT Train')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(resnet_pacs_val_acc, label='ResNet Val')\n",
    "plt.plot(vit_pacs_val_acc, label='ViT Val')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(resnet_pacs_train_loss, label='ResNet Train')\n",
    "plt.plot(vit_pacs_train_loss, label='ViT Train')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(resnet_pacs_val_loss, label='ResNet Val')\n",
    "plt.plot(vit_pacs_val_loss, label='ViT Val')\n",
    "plt.title('Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal Domain Generalization Results:\")\n",
    "print(f\"ResNet-50: Sketch Domain Accuracy: {resnet_sketch_acc:.2f}%\")\n",
    "print(f\"ViT-B/16: Sketch Domain Accuracy: {vit_sketch_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "illm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
